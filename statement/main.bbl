\begin{thebibliography}{10}

\bibitem{residual}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock {\em arXiv preprint arXiv:1602.07360}, 2016.

\bibitem{mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{prunning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem{dynamic}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  1379--1387, 2016.

\bibitem{structuredprunning}
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung.
\newblock Structured pruning of deep convolutional neural networks.
\newblock {\em ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 13(3):32, 2017.

\bibitem{singular}
Emily~L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In {\em Advances in neural information processing systems}, pages
  1269--1277, 2014.

\bibitem{tucker}
Cristina Gran{\'e}s~Santamaria.
\newblock Compression of convolutional neural networks using tucker
  decomposition.
\newblock Master's thesis, Universitat Polit{\`e}cnica de Catalunya, 2017.

\bibitem{glram}
Jieping Ye.
\newblock Generalized low rank approximations of matrices.
\newblock {\em Machine Learning}, 61(1-3):167--191, 2005.

\bibitem{block}
Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang,
  Xuehai Qian, Yu~Bai, Geng Yuan, et~al.
\newblock C ir cnn: accelerating and compressing deep neural networks using
  block-circulant weight matrices.
\newblock In {\em Proceedings of the 50th Annual IEEE/ACM International
  Symposium on Microarchitecture}, pages 395--408. ACM, 2017.

\bibitem{binary}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em Advances in neural information processing systems}, pages
  3123--3131, 2015.

\bibitem{deepcompress}
S.~{Han}, H.~{Mao}, and W.~J. {Dally}.
\newblock {Deep Compression: Compressing Deep Neural Networks with Pruning,
  Trained Quantization and Huffman Coding}.
\newblock {\em ArXiv e-prints}, October 2015.

\bibitem{ternary}
Chenzhuo Zhu, Song Han, Huizi Mao, and William~J Dally.
\newblock Trained ternary quantization.
\newblock {\em arXiv preprint arXiv:1612.01064}, 2016.

\bibitem{design}
Stuart~F Oberman and Michael~J Flynn.
\newblock Design issues in division and other floating-point operations.
\newblock {\em IEEE Transactions on computers}, 46(2):154--161, 1997.

\end{thebibliography}
